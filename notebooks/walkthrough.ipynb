{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset path: \"../data/ds000157\"\n",
      "BIDS Layout: ...ch/cs682/project/data/ds000157 | Subjects: 30 | Sessions: 0 | Runs: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up\n",
    "\"\"\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "package_dir = os.path.dirname(os.getcwd())\n",
    "parent_dir = os.path.join('..', package_dir)\n",
    "sys.path.insert(0, parent_dir)\n",
    "    \n",
    "from src import data\n",
    "\n",
    "\n",
    "layout = data.get_food_temptation_data(os.path.join(\"..\", \"data\", \"ds000157\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Authors': ['Paul A. M. Smeets',\n",
      "             'Floor M. Kroese',\n",
      "             'Catherine Evers',\n",
      "             'D. T. D. de Ridder'],\n",
      " 'BIDSVersion': '1.0.0rc3',\n",
      " 'Description': 'Thirty female subjects performed a passive viewing task with '\n",
      "                'blocks of food and nonfood images. More procedures can be '\n",
      "                'found in the publication. ',\n",
      " 'License': 'PDDL',\n",
      " 'Name': 'Block design food and nonfood picture viewing task',\n",
      " 'ReferencesAndLinks': ['http://www.ncbi.nlm.nih.gov/pubmed/23578759']}\n",
      "{'CogAtlasID': 'http://www.cognitiveatlas.org/term/id/trm_4c899211a965c',\n",
      " 'EchoTime': 0.023,\n",
      " 'FlipAngle': 72.5,\n",
      " 'MagneticFieldStrength': 3,\n",
      " 'Manufacturer': 'Philips',\n",
      " 'ManufacturerModelName': 'Achieva',\n",
      " 'RepetitionTime': 1.6,\n",
      " 'TaskDescription': 'During scanning, subjects alternately viewed 24s blocks '\n",
      "                    'of palatable food images (8 blocks) and non-food images '\n",
      "                    '(i.e., office utensils; 8 blocks), interspersed with 8–16 '\n",
      "                    's rest blocks showing a crosshair (12 s on average). '\n",
      "                    'Halfway the task there was a 10 s break. In the image '\n",
      "                    'blocks, 8 images were presented for 2.5 s each with a 0.5 '\n",
      "                    's inter-stimulus interval. All pictures were of equal '\n",
      "                    'size and displayed the (food) object on a white '\n",
      "                    'background. Food pictures were selected to arepresent '\n",
      "                    'foods that are both attractive and ‘forbidden’ (i.e., '\n",
      "                    'fattening), congruent with our definition of temptations '\n",
      "                    '[18]. Examples are shown in Supplementary Fig. S.1. A '\n",
      "                    'pilot test among 31 female students showed that on '\n",
      "                    'average the food pictures were rated as fatten- ing (M = '\n",
      "                    '5.5, SD = 1.2), as assessed on a scale ranging from 1 '\n",
      "                    '(not at all fattening) to 7 (very fattening) and as '\n",
      "                    'rather attractive (M = 5.1, SD = .9), as assessed on a '\n",
      "                    'scale from 1(not at all attractive) to 7 (very '\n",
      "                    'attractive). Food pictures were significantly more '\n",
      "                    'attractive than office utensil pictures, which were rated '\n",
      "                    'as neutral (M = 3.4, SD = 1.2; P < .01). The tempting '\n",
      "                    'nature of the food pictures was confirmed by ratings of '\n",
      "                    'ran- dom subsamples of these pictures that were obtained '\n",
      "                    'from participants in the main study after the scan (M = '\n",
      "                    '4.4, SD = .8; on a scale from 1 (not at all tempting) to '\n",
      "                    '7 (very tempting)).Supplementary data associated with '\n",
      "                    'this article can be found, in the online version, at '\n",
      "                    'http://dx.doi.org/10.1016/j.bbr.2013.03.041.',\n",
      " 'TaskName': 'Passive viewing task with blocks of food and nonfood images'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learn more about our experiment\n",
    "\"\"\"\n",
    "\n",
    "data.display_experiment_details(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diet_importance</th>\n",
       "      <th>appetite_pre</th>\n",
       "      <th>appetite_post</th>\n",
       "      <th>oral_contraceptive</th>\n",
       "      <th>days_since_menstruation</th>\n",
       "      <th>cycle_phase_covariate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LongName</th>\n",
       "      <td>Diet Importance</td>\n",
       "      <td>Pre-scan Appetite Rating (Hunger Covariate)</td>\n",
       "      <td>Post-scan Appetite Rating</td>\n",
       "      <td>Oral Contraceptive (Yes/No)</td>\n",
       "      <td>Days Since Last Menstruation (Day in Cycle)</td>\n",
       "      <td>Cycle Phase Covariate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Description</th>\n",
       "      <td>subject's self-rating of how important dieting...</td>\n",
       "      <td>subject's self-rating of appetite strength whe...</td>\n",
       "      <td>subject's self-rating of appetite strength whe...</td>\n",
       "      <td>whether or not the subject was using an oral c...</td>\n",
       "      <td>number of days since last menstruation</td>\n",
       "      <td>Menstrual Cycle phase (based on days_since_men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Units</th>\n",
       "      <td>1-5 scale, 5 being the highest</td>\n",
       "      <td>1-5 scale, 5 being the highest</td>\n",
       "      <td>1-5 scale, 5 being the highest</td>\n",
       "      <td>yes/no</td>\n",
       "      <td>days</td>\n",
       "      <td>0 = Not in phase 1 or 2 (29 days or more) or u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               diet_importance  \\\n",
       "LongName                                       Diet Importance   \n",
       "Description  subject's self-rating of how important dieting...   \n",
       "Units                           1-5 scale, 5 being the highest   \n",
       "\n",
       "                                                  appetite_pre  \\\n",
       "LongName           Pre-scan Appetite Rating (Hunger Covariate)   \n",
       "Description  subject's self-rating of appetite strength whe...   \n",
       "Units                           1-5 scale, 5 being the highest   \n",
       "\n",
       "                                                 appetite_post  \\\n",
       "LongName                             Post-scan Appetite Rating   \n",
       "Description  subject's self-rating of appetite strength whe...   \n",
       "Units                           1-5 scale, 5 being the highest   \n",
       "\n",
       "                                            oral_contraceptive  \\\n",
       "LongName                           Oral Contraceptive (Yes/No)   \n",
       "Description  whether or not the subject was using an oral c...   \n",
       "Units                                                   yes/no   \n",
       "\n",
       "                                 days_since_menstruation  \\\n",
       "LongName     Days Since Last Menstruation (Day in Cycle)   \n",
       "Description       number of days since last menstruation   \n",
       "Units                                               days   \n",
       "\n",
       "                                         cycle_phase_covariate  \n",
       "LongName                                 Cycle Phase Covariate  \n",
       "Description  Menstrual Cycle phase (based on days_since_men...  \n",
       "Units        0 = Not in phase 1 or 2 (29 days or more) or u...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>diet_success</th>\n",
       "      <th>diet_importance</th>\n",
       "      <th>appetite_pre</th>\n",
       "      <th>appetite_post</th>\n",
       "      <th>oral_contraceptive</th>\n",
       "      <th>days_since_menstruationi</th>\n",
       "      <th>cycle_phase_covariate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-01</td>\n",
       "      <td>F</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.70</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.70</td>\n",
       "      <td>5.00</td>\n",
       "      <td>yes</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-02</td>\n",
       "      <td>F</td>\n",
       "      <td>23.0</td>\n",
       "      <td>19.59</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.30</td>\n",
       "      <td>4.67</td>\n",
       "      <td>yes</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-03</td>\n",
       "      <td>F</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.00</td>\n",
       "      <td>3.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.04</td>\n",
       "      <td>4.33</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-04</td>\n",
       "      <td>F</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.13</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>4.00</td>\n",
       "      <td>yes</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-05</td>\n",
       "      <td>F</td>\n",
       "      <td>19.0</td>\n",
       "      <td>22.86</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.07</td>\n",
       "      <td>4.33</td>\n",
       "      <td>no</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id sex   age    BMI  diet_success  diet_importance  \\\n",
       "0         sub-01   F  24.0  18.70           4.3              2.0   \n",
       "1         sub-02   F  23.0  19.59           3.0              2.0   \n",
       "2         sub-03   F  19.0  19.00           3.3              2.0   \n",
       "3         sub-04   F  25.0  19.13           2.7              2.0   \n",
       "4         sub-05   F  19.0  22.86           1.7              4.0   \n",
       "\n",
       "   appetite_pre  appetite_post oral_contraceptive  days_since_menstruationi  \\\n",
       "0          3.70           5.00                yes                        93   \n",
       "1          1.30           4.67                yes                         9   \n",
       "2          2.04           4.33                yes                         0   \n",
       "3          2.96           4.00                yes                        19   \n",
       "4          4.07           4.33                 no                        14   \n",
       "\n",
       "   cycle_phase_covariate  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learn more about our subjects in the experiment\n",
    "\"\"\"        \n",
    "q, a = data.get_subject_details(layout)\n",
    "\n",
    "df_q = pd.DataFrame.from_dict(q.get_dict())\n",
    "df_a = a.get_df()\n",
    "\n",
    "display(df_q)\n",
    "display(df_a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shape:  (175, 288, 288)\n",
      "B shape:  (64, 64, 30, 375)\n",
      "Events shape:  (17, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Examine our neuro imaging data\n",
    "\"\"\"\n",
    "\n",
    "subjects = layout.get_subjects()\n",
    "T1w_img, bold_img, events = data.get_subject_data(layout, subjects[0])\n",
    "\n",
    "print(\"T shape: \", T1w_img.shape)\n",
    "print(\"B shape: \", bold_img.shape)\n",
    "print(\"Events shape: \", events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrthoSlicer3D: /home/evan/workspace/scratch/cs682/project/data/ds000157/sub-01/anat/sub-01_T1w.nii.gz (175, 288, 288)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We won't use the t_img, it seems to be a one time high res scan of the subject's brain, and was not a part\n",
    "of the experimental procedure as far as I can tell.\n",
    "\"\"\"\n",
    "T1w_img.orthoview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image datatype:  float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<OrthoSlicer3D: /home/evan/workspace/scratch/cs682/project/data/ds000157/sub-01/func/sub-01_task-passiveimageviewing_bold.nii.gz (64, 64, 30, 375)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Thoughts on what to do\n",
    "\n",
    "1. Event times seem strict, so assume the unaccounted for time is either leading in or tailing the experiment\n",
    "2. I read something about the data being processed to start at event 0.0, so I'll try calling the leftover a tail\n",
    "3. If it doesn't seem right (test with neuroimage displaying) try splitting the difference in head+tail\n",
    "\"\"\"\n",
    "\n",
    "print(\"Image datatype: \", bold_img.get_data_dtype())\n",
    "bold_img.orthoview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>onset</th>\n",
       "      <th>duration</th>\n",
       "      <th>trial_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24.10</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.1</td>\n",
       "      <td>24.06</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.2</td>\n",
       "      <td>24.07</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>111.3</td>\n",
       "      <td>24.06</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>143.3</td>\n",
       "      <td>24.06</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>179.4</td>\n",
       "      <td>24.07</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>218.5</td>\n",
       "      <td>24.04</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>251.5</td>\n",
       "      <td>24.06</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>289.6</td>\n",
       "      <td>10.00</td>\n",
       "      <td>break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>299.6</td>\n",
       "      <td>24.07</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>334.7</td>\n",
       "      <td>24.10</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>374.8</td>\n",
       "      <td>24.06</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>411.9</td>\n",
       "      <td>24.06</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>445.9</td>\n",
       "      <td>24.09</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>478.0</td>\n",
       "      <td>24.09</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>514.1</td>\n",
       "      <td>24.06</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>553.2</td>\n",
       "      <td>24.07</td>\n",
       "      <td>nonfood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    onset  duration trial_type\n",
       "0     0.0     24.10       food\n",
       "1    40.1     24.06    nonfood\n",
       "2    77.2     24.07       food\n",
       "3   111.3     24.06    nonfood\n",
       "4   143.3     24.06       food\n",
       "5   179.4     24.07    nonfood\n",
       "6   218.5     24.04       food\n",
       "7   251.5     24.06    nonfood\n",
       "8   289.6     10.00      break\n",
       "9   299.6     24.07       food\n",
       "10  334.7     24.10    nonfood\n",
       "11  374.8     24.06       food\n",
       "12  411.9     24.06    nonfood\n",
       "13  445.9     24.09       food\n",
       "14  478.0     24.09    nonfood\n",
       "15  514.1     24.06       food\n",
       "16  553.2     24.07    nonfood"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.699999999999932\n"
     ]
    }
   ],
   "source": [
    "display(events)\n",
    "\n",
    "\"\"\"\n",
    "Try and see if our understanding of the scans is aligned with the paper\n",
    "\"\"\"\n",
    "n_scans = 370  # sometimes the shape is 370, sometimes 375, I assume 370 is desired\n",
    "scan_time = 1.6  # echo time appears to be included inside scan time, not extra\n",
    "total_time = n_scans * scan_time  # 608.625 seconds, or a little past 10 minutes\n",
    "\n",
    "durations = events.onset - events.shift(periods=1).onset\n",
    "supposed_ending = events.onset.iloc[-1] + durations.median()  # 589.3\n",
    " \n",
    "# Unaccounted for time\n",
    "print(total_time - supposed_ending)\n",
    "\n",
    "# <3s sounds good, >10s is concerning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food: 180', 'nonfood: 183', 'break: 7', 'unassigned: 5']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We'll have to use the events time to try to align scans with events\n",
    "\"\"\"\n",
    "scan_assignments = data.get_scan_assignments(bold_img.get_data().shape[-1], events)\n",
    "print(list(\"{}: {}\".format(k,len(v)) for k,v in scan_assignments.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "On to the deep learning\n",
    "\n",
    "First a quick check of our helper method which should hide the complexity of the scans from us\n",
    "    data.get_machine_learning_data(layout=layout)\n",
    "    \n",
    "An f1 away from random would be very encouraging\n",
    "\"\"\"\n",
    "from src.models import sanity_check\n",
    "\n",
    "skip_svc = True\n",
    "\n",
    "if not skip_svc:\n",
    "    results = sanity_check.test_ml_data(layout=layout, limit=400)\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning data from subject 01\n",
      "Assigning data from subject 02\n",
      "Assigning data from subject 03\n",
      "MLDataset | Features: (30, 64, 64)  | Train: 762 | Validation: 218 | Test: 109\n"
     ]
    }
   ],
   "source": [
    "# TODO definitely going to want to bake in the pytorch operations\n",
    "ml_dataset = data.get_ml_dataset(layout=layout)\n",
    "# ml_dataset.normalize()\n",
    "# ml_dataset.flatten()\n",
    "print(ml_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([762, 1, 22, 48, 48]) torch.Size([762, 22, 48, 48])\n",
      "Before downsampling:  122880\n",
      "After:  50688\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ALRIGHT \n",
    "a looot of TODO here\n",
    "\n",
    "working through them, first don't flatten, second can we default to using\n",
    "pytorch as the data handler from the get go? transforms seem useful\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "# tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\n",
    "\n",
    "# Adding np.newaxis for \"channels\" which this dataset doesn't seem to have\n",
    "X_train = torch.from_numpy(ml_dataset.X_train[:,np.newaxis,:,:,:])\n",
    "y_train = torch.from_numpy(ml_dataset.y_train)\n",
    "X_val = torch.from_numpy(ml_dataset.X_val[:,np.newaxis,:,:,:])\n",
    "y_val = torch.from_numpy(ml_dataset.y_val)\n",
    "X_test = torch.from_numpy(ml_dataset.X_test[:,np.newaxis,:,:,:])\n",
    "y_test = torch.from_numpy(ml_dataset.y_test)\n",
    "\n",
    "# TODO this data is way too goddamn big, I need to downsample to survive\n",
    "#  mini-batch x channels x [optional depth] x [optional height] x width.\n",
    "#  orientation doesn't matter\n",
    "X_train = F.interpolate(\n",
    "    X_train, \n",
    "    scale_factor=3./4.,\n",
    ")\n",
    "X_val = F.interpolate(\n",
    "    X_val, \n",
    "    scale_factor=3./4,\n",
    ")\n",
    "X_test = F.interpolate(\n",
    "    X_test, \n",
    "    scale_factor=3./4,\n",
    ")\n",
    "print(X_train.shape, X_train.squeeze().shape)\n",
    "train = TensorDataset(X_train, y_train)\n",
    "val = TensorDataset(X_val, y_val)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "\n",
    "features = np.prod(X_train.shape[2:])\n",
    "print(\"Before downsampling: \", np.prod(ml_dataset.X_train.shape[1:]))\n",
    "print(\"After: \", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATASET LOADER FOR PYTORCH\"\"\"\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transform.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    train, \n",
    "    batch_size=64, \n",
    "#     transformer=data_transforms, TODO something like that\n",
    "#     sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)),\n",
    ")\n",
    "\n",
    "loader_val = DataLoader(\n",
    "    val, \n",
    "    batch_size=64, \n",
    "#     sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)),\n",
    ")\n",
    "\n",
    "loader_test = DataLoader(\n",
    "    test, \n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening:  tensor([[[[ 0,  1],\n",
      "          [ 2,  3],\n",
      "          [ 4,  5]]],\n",
      "\n",
      "\n",
      "        [[[ 6,  7],\n",
      "          [ 8,  9],\n",
      "          [10, 11]]]])\n",
      "After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"HELPER METHODS\"\"\"\n",
    "\n",
    "print_every = 1\n",
    "\n",
    "def train_part34(model, optimizer, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(\"epoch\", e)\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            print(\"timestep\", t, (x.shape, y.shape))\n",
    "            # TODO float32? use x.double()?\n",
    "            \n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "                \n",
    "            print(x.shape, y.shape)\n",
    "            scores = model(x)\n",
    "            print(\"OKAY\")\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()\n",
    "                \n",
    "def check_accuracy_part34(loader, model):\n",
    "    print('Checking accuracy on validation set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # TODO fucking dtypes\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "     \n",
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "def test_flatten():\n",
    "    x = torch.arange(12).view(2, 1, 3, 2)\n",
    "    print('Before flattening: ', x)\n",
    "    print('After flattening: ', flatten(x))\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "    \n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Sequential TEST\n",
    "\n",
    "Ah okay, huge fucking investigation on loss\n",
    "\n",
    "look at other examples of handling pytorch anyhow\n",
    "\n",
    "\"\"\"\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "num_classes = 2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    # TODO jesus christ the allocation here is massive, I need to shrink something\n",
    "    nn.Linear(features, hidden_layer_size),  \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, num_classes),\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    momentum=0.9, \n",
    "    nesterov=True,\n",
    ")\n",
    "\n",
    "# train_part34(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "timestep 0 (torch.Size([64, 1, 22, 48, 48]), torch.Size([64]))\n",
      "torch.Size([64, 1, 22, 48, 48]) torch.Size([64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 6488064000 bytes. Error code 12 (Cannot allocate memory)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6b9b9e3b33d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mtrain_part34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-1f2b93dd6cb4>\u001b[0m in \u001b[0;36mtrain_part34\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OKAY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tools/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tools/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tools/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tools/anaconda3/envs/cs682/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    478\u001b[0m                             self.dilation, self.groups)\n\u001b[1;32m    479\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 480\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 6488064000 bytes. Error code 12 (Cannot allocate memory)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "factor = 8\n",
    "in_channel = 1\n",
    "channel_1 = 4 * factor\n",
    "channel_2 = 8 * factor\n",
    "channel_3 = 8 * factor\n",
    "channel_4 = 16 * factor\n",
    "\n",
    "\n",
    "# torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "model = nn.Sequential(\n",
    "#     nn.Conv3d(in_channel, channel_1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.Conv3d(in_channel, channel_1, kernel_size=7, stride=2, padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv3d(channel_1, channel_2, kernel_size=5, stride=1, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool3d(kernel_size=2),\n",
    "    nn.Dropout(p=0.4),\n",
    "    \n",
    "    nn.Conv3d(channel_2, channel_3, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(channel_3, channel_4, kernel_size=3, stride=1, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Dropout(p=0.4),\n",
    "\n",
    "    Flatten(),\n",
    "    nn.Linear(1152, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, num_classes),\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate,\n",
    "    momentum=0.9, \n",
    "    nesterov=True,\n",
    ")\n",
    "\n",
    "train_part34(model, optimizer, epochs=1)\n",
    "\n",
    "# RuntimeError: Expected 4-dimensional input for 4-dimensional weight 64 64 5 5, but got 5-dimensional input of size [64, 64, 5, 12, 12] instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available Modalities : T1w, bold\n",
    "# TODO need to understand what the difference is, can I just use one?\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "# img_path = os.path.join(os.getcwd(), ds_path, 'sub-01', 'func', b_img.filename)\n",
    "# plotting.plot_img(nibabel_b_img)\n",
    "\n",
    "# layout.to_df().path.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'train'\n",
    "VAL = 'val'\n",
    "TEST = 'test'\n",
    "\n",
    "# TODO shit, the image does not seem to fit at all.\n",
    "\n",
    "# VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "data_transforms = {\n",
    "    TRAIN: transforms.Compose([\n",
    "        # Data augmentation is a good practice for the train set\n",
    "        # Here, we randomly crop the image to 224x224 and\n",
    "        # randomly flip it horizontally. \n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    VAL: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    TEST: transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(\n",
    "        os.path.join(data_dir, x), \n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=8,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [TRAIN, VAL, TEST]\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n",
    "\n",
    "for x in [TRAIN, VAL, TEST]:\n",
    "    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n",
    "    \n",
    "print(\"Classes: \")\n",
    "class_names = image_datasets[TRAIN].classes\n",
    "print(image_datasets[TRAIN].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana\n",
    "# TODO use nilearn?\n",
    "\n",
    "from nilearn.datasets import fetch_haxby\n",
    "data_files = fetch_haxby()\n",
    "\n",
    "# Load behavioral data\n",
    "import pandas as pd\n",
    "behavioral = pd.read_csv(data_files.session_target[0], sep=\" \")\n",
    "\n",
    "# Restrict to face and house conditions\n",
    "conditions = behavioral['labels']\n",
    "condition_mask = conditions.isin(['face', 'house'])\n",
    "\n",
    "# Split data into train and test samples, using the chunks\n",
    "condition_mask_train = (condition_mask) & (behavioral['chunks'] <= 6)\n",
    "condition_mask_test = (condition_mask) & (behavioral['chunks'] > 6)\n",
    "\n",
    "# Apply this sample mask to X (fMRI data) and y (behavioral labels)\n",
    "# Because the data is in one single large 4D image, we need to use\n",
    "# index_img to do the split easily\n",
    "from nilearn.image import index_img\n",
    "func_filenames = data_files.func[0]\n",
    "X_train = index_img(func_filenames, condition_mask_train)\n",
    "X_test = index_img(func_filenames, condition_mask_test)\n",
    "y_train = conditions[condition_mask_train]\n",
    "y_test = conditions[condition_mask_test]\n",
    "\n",
    "# Compute the mean epi to be used for the background of the plotting\n",
    "from nilearn.image import mean_img\n",
    "background_img = mean_img(func_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg\n",
    "\n",
    "model = vgg.vgg11()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nibabel as nib\n",
    "from nibabel.testing import data_path\n",
    "from nilearn import image\n",
    "from nilearn import plotting\n",
    "\n",
    "\n",
    "img_path = \"/home/evan/Downloads/ds000221_R1.0.0/sub-010004/ses-02/func/sub-010004_ses-02_task-rest_acq-AP_run-01_bold.nii.gz\"\n",
    "# img_path = os.path.join(data_path, 'example4d.nii.gz')\n",
    "# img = nib.load(img_path)\n",
    "# # print(img)\n",
    "# print(img.shape)\n",
    "\n",
    "# plotting.plot_img(example_filename)\n",
    "\n",
    "# img3d = image.index_img(img_path, 0)\n",
    "# print(img3d.shape)\n",
    "# plotting.plot_stat_map(img3d)\n",
    "# plotting.plot_img(img3d)\n",
    "# plt.show()\n",
    "\n",
    "print(\"bigboi\")\n",
    "all_images = list(image.iter_img(img_path))\n",
    "for img in all_images[:5]:\n",
    "    # img is now an in-memory 3D img\n",
    "    plotting.plot_stat_map(img, threshold=3, display_mode=\"z\", cut_coords=1, colorbar=False)\n",
    "plotting.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
